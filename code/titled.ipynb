{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WkKKPrEqNU8u",
        "outputId": "d4f94995-789b-45b7-f3fd-6869238d54a3"
      },
      "outputs": [],
      "source": [
        "!pip install ultralytics\n",
        "# Install required packages in Google Colab\n",
        "!pip install filterpy\n",
        "!pip install scipy\n",
        "!pip install scikit-learn\n",
        "!pip install torch torchvision\n",
        "!pip install ultralytics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#   1. Approach 1 ( Pretrained ResNet based model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. Import Dependencies and Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "from torchvision import transforms\n",
        "from collections import namedtuple\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "from ultralytics import YOLO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Video Preprocessing and Player Detection using YOLO\n",
        "Assuming we have the YOLO model available for object detection, we'll load it and process the video frames.\n",
        "\n",
        "Input: broadcast.mp4, tacticam.mp4\n",
        "\n",
        "Output: Bounding boxes and confidence scores for players\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Assume we have a YOLOv4 or YOLOv5 model pre-trained for player detection\n",
        "class YOLOModel:\n",
        "    def __init__(self, model_path):\n",
        "        self.model = torch.hub.load('ultralytics/yolov5', 'yolov5s')  # Using YOLOv5s as an example\n",
        "\n",
        "    def detect(self, frame):\n",
        "        results = self.model(frame)\n",
        "        return results.pandas().xywh  # Return the bounding boxes (center_x, center_y, width, height)\n",
        "\n",
        "# Load the video files\n",
        "video_broadcast = cv2.VideoCapture(\"broadcast.mp4\")\n",
        "video_tacticam = cv2.VideoCapture(\"tacticam.mp4\")\n",
        "\n",
        "yolo_model = YOLOModel('yolov5')\n",
        "\n",
        "def process_frame(video_capture, frame_id):\n",
        "    success, frame = video_capture.read()\n",
        "    if not success:\n",
        "        return None\n",
        "    detections = yolo_model.detect(frame)\n",
        "    return detections\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Feature Extraction using ResNet-101\n",
        "We use a pre-trained ResNet-101 model to extract embeddings for the detected players. This helps in feature extraction from the cropped player images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class FeatureExtractor:\n",
        "    def __init__(self):\n",
        "        self.model = models.resnet101(pretrained=True)\n",
        "        self.model = nn.Sequential(*list(self.model.children())[:-1])  # Remove the final classification layer\n",
        "        self.model.eval()\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.ToPILImage(),\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ])\n",
        "\n",
        "    def extract_embedding(self, image):\n",
        "        image = self.transform(image)\n",
        "        image = image.unsqueeze(0)\n",
        "        with torch.no_grad():\n",
        "            embedding = self.model(image)\n",
        "        return embedding.flatten()\n",
        "\n",
        "feature_extractor = FeatureExtractor()\n",
        "\n",
        "def extract_player_features(detections, frame):\n",
        "    features = []\n",
        "    for _, row in detections.iterrows():\n",
        "        x1, y1, x2, y2 = row['xmin'], row['ymin'], row['xmax'], row['ymax']\n",
        "        player_crop = frame[int(y1):int(y2), int(x1):int(x2)]\n",
        "        feature = feature_extractor.extract_embedding(player_crop)\n",
        "        features.append(feature)\n",
        "    return features\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. Cosine Similarity Calculation for Matching\n",
        "Cosine similarity is used to compare player embeddings between two frames (one from broadcast.mp4 and one from tacticam.mp4)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_similarity(features_broadcast, features_tacticam):\n",
        "    cosine_sim = cosine_similarity(features_broadcast, features_tacticam)\n",
        "    return cosine_sim\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5. Hungarian Algorithm for Optimal Matching\n",
        "The Hungarian algorithm is used to find the optimal one-to-one assignment of players between the two frames based on the similarity matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def hungarian_algorithm(similarity_matrix):\n",
        "    row_ind, col_ind = linear_sum_assignment(-similarity_matrix)  # Maximize similarity\n",
        "    return row_ind, col_ind\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6. Final Matching Output and Evaluation\n",
        "After calculating similarities and using the Hungarian algorithm, we output the optimal player matches.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def match_players(video_broadcast, video_tacticam):\n",
        "    frame_id = 0\n",
        "    while True:\n",
        "        broadcast_detections = process_frame(video_broadcast, frame_id)\n",
        "        tacticam_detections = process_frame(video_tacticam, frame_id)\n",
        "\n",
        "        if broadcast_detections is None or tacticam_detections is None:\n",
        "            break\n",
        "\n",
        "        broadcast_features = extract_player_features(broadcast_detections, video_broadcast.read()[1])\n",
        "        tacticam_features = extract_player_features(tacticam_detections, video_tacticam.read()[1])\n",
        "\n",
        "        similarity_matrix = calculate_similarity(broadcast_features, tacticam_features)\n",
        "        row_ind, col_ind = hungarian_algorithm(similarity_matrix)\n",
        "\n",
        "        # Display matched players (for debugging)\n",
        "        for i, j in zip(row_ind, col_ind):\n",
        "            print(f\"Player {i} in Broadcast.mp4 is matched with Player {j} in Tacticam.mp4\")\n",
        "\n",
        "        frame_id += 1\n",
        "\n",
        "# Running the matching\n",
        "match_players(video_broadcast, video_tacticam)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#   2. Approach 2 ( Pretrained Siamese Network model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. Import Necessary Libraries\n",
        "We will import the necessary libraries for video processing, deep learning, and optimization.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import cv2\n",
        "import torch\n",
        "import numpy as np\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from torchvision import transforms\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Load Pretrained Siamese Network\n",
        "For simplicity, we'll assume you have access to a pretrained Siamese network that is designed to compare player appearances directly. A basic Siamese network will accept two images and output embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SiameseNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SiameseNetwork, self).__init__()\n",
        "        # For simplicity, we assume a custom CNN architecture for the Siamese network.\n",
        "        # A typical architecture would involve two identical CNNs that share weights.\n",
        "\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Linear(256 * 28 * 28, 512),  # Assuming the image is 224x224\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(512, 128),  # Final embedding size is 128\n",
        "        )\n",
        "\n",
        "    def forward_once(self, x):\n",
        "        x = self.conv_layers(x)\n",
        "        x = x.view(x.size(0), -1)  # Flatten the output\n",
        "        x = self.fc_layers(x)\n",
        "        return x\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        # Forward pass for two images\n",
        "        output1 = self.forward_once(x1)\n",
        "        output2 = self.forward_once(x2)\n",
        "        return output1, output2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###  3. Video Preprocessing & Player Detection Using YOLO\n",
        "We will use YOLO to detect players in both broadcast.mp4 and tacticam.mp4. We then crop out the bounding boxes for each player and send them to the Siamese network.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class YOLOModel:\n",
        "    def __init__(self):\n",
        "        # Loading YOLO model (YOLOv5 as an example)\n",
        "        self.model = torch.hub.load('ultralytics/yolov5', 'yolov5s')  # You can load a larger model if needed\n",
        "\n",
        "    def detect(self, frame):\n",
        "        results = self.model(frame)\n",
        "        # Return player bounding boxes in xywh format\n",
        "        return results.pandas().xywh\n",
        "\n",
        "# Video capture initialization\n",
        "video_broadcast = cv2.VideoCapture(\"broadcast.mp4\")\n",
        "video_tacticam = cv2.VideoCapture(\"tacticam.mp4\")\n",
        "yolo_model = YOLOModel()\n",
        "\n",
        "def process_frame(video_capture):\n",
        "    success, frame = video_capture.read()\n",
        "    if not success:\n",
        "        return None\n",
        "    detections = yolo_model.detect(frame)\n",
        "    return detections\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. Extract Features Using Siamese Network\n",
        "The player bounding boxes will be cropped, and these cropped images will be passed to the Siamese network for feature extraction.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class FeatureExtractor:\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.ToPILImage(),\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ])\n",
        "\n",
        "    def extract_embedding(self, image):\n",
        "        image = self.transform(image)\n",
        "        image = image.unsqueeze(0)  # Add batch dimension\n",
        "        with torch.no_grad():\n",
        "            embedding = self.model(image)\n",
        "        return embedding[0]  # Return the first (and only) embedding\n",
        "\n",
        "# Initialize the model and feature extractor\n",
        "siamese_model = SiameseNetwork()\n",
        "feature_extractor = FeatureExtractor(siamese_model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5. Calculate Cosine Similarity for Matching\n",
        "Once we extract the embeddings, we calculate the cosine similarity between the embeddings of players from the two videos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_similarity(features_broadcast, features_tacticam):\n",
        "    # Cosine similarity between each pair of players (broadcast vs. tacticam)\n",
        "    cosine_sim = cosine_similarity(features_broadcast, features_tacticam)\n",
        "    return cosine_sim\n",
        "def hungarian_algorithm(similarity_matrix):\n",
        "    row_ind, col_ind = linear_sum_assignment(-similarity_matrix)  # Maximize similarity by minimizing the negative\n",
        "    return row_ind, col_ind\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6. Final Matching Output and Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def match_players(video_broadcast, video_tacticam):\n",
        "    frame_id = 0\n",
        "    while True:\n",
        "        # Process frames from both videos\n",
        "        broadcast_detections = process_frame(video_broadcast)\n",
        "        tacticam_detections = process_frame(video_tacticam)\n",
        "\n",
        "        if broadcast_detections is None or tacticam_detections is None:\n",
        "            break\n",
        "\n",
        "        # Extract features for players in both videos\n",
        "        broadcast_features = []\n",
        "        tacticam_features = []\n",
        "\n",
        "        for _, row in broadcast_detections.iterrows():\n",
        "            x1, y1, x2, y2 = row['xmin'], row['ymin'], row['xmax'], row['ymax']\n",
        "            player_crop = video_broadcast.read()[1][int(y1):int(y2), int(x1):int(x2)]\n",
        "            feature = feature_extractor.extract_embedding(player_crop)\n",
        "            broadcast_features.append(feature)\n",
        "\n",
        "        for _, row in tacticam_detections.iterrows():\n",
        "            x1, y1, x2, y2 = row['xmin'], row['ymin'], row['xmax'], row['ymax']\n",
        "            player_crop = video_tacticam.read()[1][int(y1):int(y2), int(x1):int(x2)]\n",
        "            feature = feature_extractor.extract_embedding(player_crop)\n",
        "            tacticam_features.append(feature)\n",
        "\n",
        "        # Convert list to numpy array for cosine similarity\n",
        "        broadcast_features = np.array([f.numpy() for f in broadcast_features])\n",
        "        tacticam_features = np.array([f.numpy() for f in tacticam_features])\n",
        "\n",
        "        # Calculate similarity and apply Hungarian algorithm\n",
        "        similarity_matrix = calculate_similarity(broadcast_features, tacticam_features)\n",
        "        row_ind, col_ind = hungarian_algorithm(similarity_matrix)\n",
        "\n",
        "        # Output the matched players\n",
        "        for i, j in zip(row_ind, col_ind):\n",
        "            print(f\"Player {i} in Broadcast.mp4 is matched with Player {j} in Tacticam.mp4\")\n",
        "\n",
        "        frame_id += 1\n",
        "\n",
        "# Running the player matching function\n",
        "match_players(video_broadcast, video_tacticam)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "sai",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
